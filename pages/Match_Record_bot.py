#ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)
from langchain.chains import LLMChain
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.messages import ChatMessage
import pandas as pd
import streamlit as st
import openai
#í™˜ê²½ë³€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¸íŒ…
from dotenv import load_dotenv
import os
#API í™œìš©ë³„ í‚¤
# myOpenAI_key = st.secrets["myOpenAI"]
load_dotenv()
myOpenAI_key = os.getenv("myOpenAI")


#Streamlit : í—¤ë”
st.set_page_config(layout="wide")
st.header("Match Record BotğŸ¤–ğŸ§ ğŸ‡¦ğŸ‡®ğŸ‘¾")
st.markdown("23/24ë…„ë„ ìœ ëŸ½ 5ëŒ€ ë¦¬ê·¸ ê²½ê¸°ê²°ê³¼ ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì±—ë´‡ í˜ì´ì§€")

#Streamlit : ë°ì´í„° í˜¸ì¶œ
raw_data = pd.read_csv("./useData/matchResult_bot_data.csv")
st.dataframe(raw_data, hide_index=True, use_container_width=True)

#ì±—ë´‡ì˜ì—­
#ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜ì—­
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if user_input := st.chat_input("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    # LLM ì‚¬ìš© í•˜ì—¬ AI ë‹µë³€ ìƒì„±
    #ë­ì²´ì¸
    # â”” ë­ì²´ì¸ MODEL ìƒì„±
    # ë°ì´í„° ë¡œë“œ
    loader = CSVLoader('./useData/matchResult_bot_data.csv')
    matchResult = loader.load()
    # í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(separators = "\n",
                                                chunk_size = 1000, # 1000ìì”© split
                                                chunk_overlap=0) # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì—†ê²Œ
    matchPrediction = text_splitter.split_documents(matchResult)
    # Chromaì— ì €ì¥
    vectorstore = Chroma.from_documents(documents = matchPrediction, embedding=OpenAIEmbeddings())
    # ê²€ìƒ‰ ì„¤ì •
    retriever = vectorstore.as_retriever()
    # í…œí”Œë¦¿ ë§Œë“¤ê¸°
    template = """
    ë‹µë³€ ê³¼ì •ì— ìˆì–´ ë‹¤ìŒì˜ ìš”ì²­ì— ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ìš”ì²­:
    1. ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
    2. team ì´ë¦„ì— ëŒ€í•´ í•œê¸€ë¡œ ì§ˆë¬¸í•´ë„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ì†ì„±ê³¼ ê·¸ì— ëŒ€í•œ ë‹µì„ ì°¾ìœ¼ì„¸ìš”.
    3. í•´ë‹¹ ë°ì´í„° ë‚´ì—ì„œë§Œ ë‹µë³€ì„ ì§„í–‰í•˜ë©°, ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
    4. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ í•´ì£¼ì„¸ìš”.
    \n\nCONTEXT: {question}\n\nSUMMARY:"""
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_template(template)
    prompt
    # llm ê°ì²´ ìƒì„±
    llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-0125', api_key = myOpenAI_key)
    # chain ìƒì„±
    chain = ({"question" : RunnablePassthrough()} | prompt | llm | StrOutputParser())

    #ì§ˆë¬¸ ì „ë‹¬
    try:
        #responseì— ì…ë ¥ê°’ì´ ëª¨ë‘ ë‹´ê¹€.
        response = chain.invoke(user_input)
        # response ì˜ ë”•ì…”ë„ˆë¦¬ì—ëŠ” response['intermediate_steps'] í‚¤ê°€ ìˆê±°ë‚˜ ì—†ì„ ë•Œê°€ ìˆìœ¼ë©°, ì´ëŠ” ì§ˆë¬¸ìœ í˜•ì— ë”°ë¼ ìƒì´í•¨
        # response ì˜ ë”•ì…”ë„ˆë¦¬ response['intermediate_steps'] ê°’ì´ 0ì¼ ë–„ëŠ” ì§ˆë¬¸ê³¼ ë‹µë§Œ ì¶œë ¥ í‘œì‹œ
        if len(response['intermediate_steps']) == 0: #reponse > intermediate_stepsê¸¸ì´ê°€ 0ì´ë©´ ì•„ë˜ ì½”ë“œ ì‹¤í–‰
            myAsk = response["input"] # response > inputì€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ê°’
            AIresponse = response["output"] # response > outputì€ aiê°€ ë‹µë³€í•œ ê°’
            st.session_state["messages"].append(ChatMessage(role="user", content=myAsk)) #ëŒ€í™”ë‚´ìš© ì €ì¥
            # AI ë‹µë³€
            with st.chat_message("assistant"):
                st.write(AIresponse) #ai ë‹µë³€
                st.session_state["messages"].append(ChatMessage(role="assistant", content=AIresponse)) #ëŒ€í™”ë‚´ìš© ì €ì¥
        # response ì˜ ë”•ì…”ë„ˆë¦¬ response['intermediate_steps'] ê°’ì´ 1ì¼ ë–„ëŠ”  ë‘ê°€ì§€ ê²½ìš°ë¡œ ë‚˜ëˆ”. (1) .pltê°€ í¬í•¨ë˜ì–´ ìˆì„ ë•Œ  (2) pltê°€ í¬í•¨ì•ˆë˜ì–´ ìˆì„ë•Œ
        elif len(response['intermediate_steps']) == 1:
            myAsk = response["input"] # response > inputì€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ê°’
            AIresponse = response["output"] # response > outputì€ aiê°€ ë‹µë³€í•œ ê°’
            st.session_state["messages"].append(ChatMessage(role="user", content=myAsk)) #ëŒ€í™”ë‚´ìš© ì €ì¥
            visual_query = response['intermediate_steps'][0][0].tool_input['query']  # reponse > intermediate_stepsê¸¸ì´ê°€ 0ì´ë©´ ì•„ë˜ ì½”ë“œ ì‹¤í–‰
            if "plt." not in visual_query: #(1) .pltê°€ í¬í•¨ë˜ì–´ ìˆì„ ë•Œ  : ì§ˆë¬¸ê³¼ ë‹µë§Œ ì¶œë ¥
                # AI ë‹µë³€
                with st.chat_message("assistant"):
                    st.write(AIresponse)
                    st.session_state["messages"].append(ChatMessage(role="assistant", content=AIresponse))
            else: #(2) pltê°€ í¬í•¨ì•ˆë˜ì–´ ìˆì„ë•Œ ì§ˆë¬¸ê³¼ ë‹µ ì´ë¯¸ì§€ ì¶œë ¥
                save_img = visual_query + "\nplt.savefig('./useData/save_fig_default.png')" #visual ì¿¼ë¦¬ëŠ” openaiê°€ ì‹œê°í™”ì°¨íŠ¸ë¥¼ ê·¸ë ¤ì¤€ íŒŒì´ì¬ ì½”ë“œê°€ ë‹´ê²¨ìˆìŒ. ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±í•˜ê³  ë‹µë³€ì—ì„œ í•¨ê¼ë³´ì—¬ì¤Œ
                df = raw_data.copy() #ì‹œê°í™” íŒŒì´ì¬ ì½”ë“œë¥¼ ì¬ì‹¤í–‰í•˜ê¸°ìœ„í•´ í•´ë‹¹ì½”ë“œì™€ ì•„ë˜ì½”ë“œ ì‹¤í–‰
                finish_img = exec(save_img)
                # AI ë‹µë³€
                with st.chat_message("assistant"):
                    st.write(AIresponse) #ë‹µë³€
                    st.image("./useData/save_fig_default.png") #ìœ„ì—ì„œ ì €ì¥í•œ ì‹œê°í™”ì°¨íŠ¸ ì¶œë ¥
                    st.session_state["messages"].append(ChatMessage(role="assistant", content=AIresponse))

    #ì˜ˆì™¸ì²˜ë¦¬
    except openai.BadRequestError:
        st.session_state["messages"].append(ChatMessage(role="assistant", content="ì œê²Œ ì£¼ì‹  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ í† í°ì´ ê¸°ì¤€ì¹˜ë³´ë‹¤ ì´ˆê³¼ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ ë¶€íƒí•´ìš”."))
    except openai.FileNotFoundError:
        st.session_state["messages"].append(ChatMessage(role="assistant", content="ì œê²Œ ì£¼ì‹  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ í† í°ì´ ê¸°ì¤€ì¹˜ë³´ë‹¤ ì´ˆê³¼ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ ë¶€íƒí•´ìš”."))
    except openai.AuthenticationError:
        st.session_state["messages"].append(ChatMessage(role="assistant", content="API í‚¤ë¥¼ í™•ì¸ë¶€íƒë“œë ¤ìš”."))

    with st.container():
        st.write(st.session_state["messages"])