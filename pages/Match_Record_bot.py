#ë¼ì´ë¸ŒëŸ¬ë¦¬
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
#import os

from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)
from langchain.chains import LLMChain
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
import pandas as pd
import streamlit as st
import openai
#í™˜ê²½ë³€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¸íŒ…
#from dotenv import load_dotenv
#import os
#API í™œìš©ë³„ í‚¤
myOpenAI_key = st.secrets["OPENAI_API_KEY"]
#load_dotenv()
#myOpenAI_key = os.getenv("OPENAI_API_KEY") 
# myOpenAI_key = os.getenv("myOpenAI")
#Streamlit : í—¤ë”
st.set_page_config(layout="wide")
tab_1, tab2 = st.tabs(["Chat-bot", "Code"])
with tab_1:
    st.header("Match Record BotğŸ¤–ğŸ§ ğŸš€") #ë¡œë´‡_ì–¼êµ´::ë‡Œ::ì•µê·ˆë¼_ì„¬_ê¹ƒë°œ::ìš°ì£¼_ì¹¨ëµì:
    st.markdown("23/24ë…„ë„ ìœ ëŸ½ 5ëŒ€ ë¦¬ê·¸ ê²½ê¸°ê²°ê³¼ ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì±—ë´‡ í˜ì´ì§€")
    #Streamlit : ë°ì´í„° í˜¸ì¶œ
    raw_data = pd.read_csv("./useData/matchResult_bot_data.csv")
    st.dataframe(raw_data, hide_index=True, use_container_width=True)
    #ì±—ë´‡ì˜ì—­
    #ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜ì—­
    if user_input := st.chat_input("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
        st.chat_message("user").write(f"{user_input}")
        # LLM ì‚¬ìš© í•˜ì—¬ AI ë‹µë³€ ìƒì„±
        #ë­ì²´ì¸
        # â”” ë­ì²´ì¸ MODEL ìƒì„±
        # ë°ì´í„° ë¡œë“œ
        loader = CSVLoader('./useData/matchResult_bot_data.csv')
        matchResult = loader.load()
        # í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°
        text_splitter = RecursiveCharacterTextSplitter(separators = "\n",
                                                    chunk_size = 100, # 1000ìì”© split
                                                    chunk_overlap=0) # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì—†ê²Œ
        matchPrediction = text_splitter.split_documents(matchResult)
        # Chromaì— ì €ì¥
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(matchPrediction, embeddings)
        # ê²€ìƒ‰ ì„¤ì •
        retriever = vectorstore.as_retriever()
        # í…œí”Œë¦¿ ë§Œë“¤ê¸°
        template = """
        ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ë‹µë³€ ê³¼ì •ì— ìˆì–´ ë‹¤ìŒì˜ ìš”ì²­ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”.
        ìš”ì²­:
        1. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        2. team ì´ë¦„ì— ëŒ€í•´ í•œê¸€ë¡œ ì§ˆë¬¸í•´ë„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ì†ì„±ê³¼ ê·¸ì— ëŒ€í•œ ë‹µì„ ì°¾ìœ¼ì„¸ìš”.
        3. ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì¹œì ˆí•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
        4. ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
        5. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ í•´ì£¼ì„¸ìš”.
        \n\nCONTEXT: {question}\n\nSUMMARY:"""
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_template(template)
        # llm ê°ì²´ ìƒì„±
        llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-0125', api_key = myOpenAI_key)
        # chain ìƒì„±
        chain = ({"question" : RunnablePassthrough()} | prompt | llm | StrOutputParser())
        #ì§ˆë¬¸ ì „ë‹¬
            #responseì— ì…ë ¥ê°’ì´ ëª¨ë‘ ë‹´ê¹€.
        response = chain.invoke(user_input)
        myAsk = user_input
        AIresponse = response # response > outputì€ aiê°€ ë‹µë³€í•œ ê°’
        # AI ë‹µë³€
        with st.chat_message("assistant"):
            st.write(AIresponse) #ai ë‹µë³€
            st.session_state["messages"].append(ChatMessage(role="assistant", content=AIresponse)) #ëŒ€í™”ë‚´ìš© ì €ì¥
        #ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜ì—­
        if "messages" not in st.session_state:
            st.session_state["messages"] = []
with tab_2:
    source_code='''st.set_page_config(layout="wide")
st.header("Match Record BotğŸ¤–ğŸ§ ğŸš€") #ë¡œë´‡_ì–¼êµ´::ë‡Œ::ì•µê·ˆë¼_ì„¬_ê¹ƒë°œ::ìš°ì£¼_ì¹¨ëµì:
st.markdown("23/24ë…„ë„ ìœ ëŸ½ 5ëŒ€ ë¦¬ê·¸ ê²½ê¸°ê²°ê³¼ ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì±—ë´‡ í˜ì´ì§€")
#Streamlit : ë°ì´í„° í˜¸ì¶œ
raw_data = pd.read_csv("./useData/matchResult_bot_data.csv")
st.dataframe(raw_data, hide_index=True, use_container_width=True)
#ì±—ë´‡ì˜ì—­
#ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜ì—­
if user_input := st.chat_input("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    # LLM ì‚¬ìš© í•˜ì—¬ AI ë‹µë³€ ìƒì„±
    #ë­ì²´ì¸
    # â”” ë­ì²´ì¸ MODEL ìƒì„±
    # ë°ì´í„° ë¡œë“œ
    loader = CSVLoader('./useData/matchResult_bot_data.csv')
    matchResult = loader.load()
    # í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(separators = "\n",
                                                chunk_size = 100, # 1000ìì”© split
                                                chunk_overlap=0) # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì—†ê²Œ
    matchPrediction = text_splitter.split_documents(matchResult)
    # Chromaì— ì €ì¥
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(matchPrediction, embeddings)
    # ê²€ìƒ‰ ì„¤ì •
    retriever = vectorstore.as_retriever()
    # í…œí”Œë¦¿ ë§Œë“¤ê¸°
    template = """
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ë‹µë³€ ê³¼ì •ì— ìˆì–´ ë‹¤ìŒì˜ ìš”ì²­ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”.
    ìš”ì²­:
    1. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    2. team ì´ë¦„ì— ëŒ€í•´ í•œê¸€ë¡œ ì§ˆë¬¸í•´ë„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ì†ì„±ê³¼ ê·¸ì— ëŒ€í•œ ë‹µì„ ì°¾ìœ¼ì„¸ìš”.
    3. ëª¨ë¥´ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì¹œì ˆí•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
    4. ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
    5. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ í•´ì£¼ì„¸ìš”.
    \n\nCONTEXT: {question}\n\nSUMMARY:"""
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_template(template)
    # llm ê°ì²´ ìƒì„±
    llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-0125', api_key = myOpenAI_key)
    # chain ìƒì„±
    chain = ({"question" : RunnablePassthrough()} | prompt | llm | StrOutputParser())
    #ì§ˆë¬¸ ì „ë‹¬
        #responseì— ì…ë ¥ê°’ì´ ëª¨ë‘ ë‹´ê¹€.
    response = chain.invoke(user_input)
    myAsk = user_input
    AIresponse = response # response > outputì€ aiê°€ ë‹µë³€í•œ ê°’
    # AI ë‹µë³€
    with st.chat_message("assistant"):
        st.write(AIresponse) #ai ë‹µë³€
        st.session_state["messages"].append(ChatMessage(role="assistant", content=AIresponse)) #ëŒ€í™”ë‚´ìš© ì €ì¥
    #ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜ì—­
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
'''
st.code(source_code, language='python')
