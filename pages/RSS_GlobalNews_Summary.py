#ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ì˜
import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup 
import schedule
import requests
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate

#í™˜ê²½ë³€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ì„¸íŒ…
import os
#from dotenv import load_dotenv
#load_dotenv()
global myOpenAI_key
global myZenrows
myOpenAI_key=st.secrets["myOpenAI"]
myZenrows=st.secrets["myZenrows"]

# OPENAI_API_KEY = os.getenv("myOpenAI")

#í•¨ìˆ˜ ì •ì˜ë¶€ 
def main():
    #ê¸€ë¡œë²Œ ë³€ìˆ˜ ì„¤ì •
    global eng_newsList
    global ger_newsList
    global spa_newsList
    global ita_newsList
    global eng_newsList_url
    global ger_newsList_url
    global spa_newsList_url
    global ita_newsList_url
    global eng_all_list
    global other_all_list
    #ê¸°ì‚¬ì œëª© 
    eng_newsList = list()
    ger_newsList = list()
    spa_newsList = list()
    ita_newsList = list()
    # ê¸°ì‚¬ë³„ë§í¬
    eng_newsList_url = list()
    ger_newsList_url = list()
    spa_newsList_url = list()
    ita_newsList_url = list()

    #ì˜êµ­ë‰´ìŠ¤============================================
    eng_new = "https://www.bbc.com/sport/football"
    eng_get_info = requests.get(eng_new)
    eng_all_data = eng_get_info.text
    myparser = BeautifulSoup(eng_all_data, 'html.parser')  # í•´ë‹¹ ë§í¬ html íŒŒì‹±
    eng_articles_area = myparser.select("div.ssrcss-tq7xfh-PromoContent.exn3ah99")
    for v in eng_articles_area:
        #ê¸°ì‚¬ ì œëª©
        eng_news_title = v.text
        #ê¸°ì‚¬ë§í¬ ì¶”ì¶œ > href í™œìš©í•˜ì—¬ ì¶”ì¶œëœ ê°’ê³¼ ê¸°ì¡´ ë„ë©”ì¸ url í•©ì„±
        eng_news_short_link = v.select_one("div.ssrcss-1f3bvyz-Stack.e1y4nx260 > a").get("href")
        eng_news_full_link = "https://www.bbc.com" + eng_news_short_link
        eng_newsList.append(eng_news_title)
        eng_newsList = eng_newsList[:10]
        eng_newsList_url.append(eng_news_full_link)
        eng_newsList_url = eng_newsList_url[:10]

    #ë…ì¼ë‰´ìŠ¤============================================
    #ë§í¬ ì ‘ì†
    german_new = "https://sportbild.bild.de/fussball/startseite/fussball/home-33017580.sport.html"

    german_get_info = requests.get(german_new)
    german_all_data = german_get_info.text
    myparser = BeautifulSoup(german_all_data, 'html.parser')

    german_articles_area = myparser.select("article.stage-teaser.mini-quad.article")
    for v in german_articles_area :

        genman_news_title = v.text.strip()
        genman_news_link = "https://sportbild.bild.de/" +v.select_one("a").get("href")
        ger_newsList.append(genman_news_title)
        ger_newsList = ger_newsList[:10]
        ger_newsList_url.append(genman_news_link)
        ger_newsList_url = ger_newsList_url[:10]

    #ìŠ¤í˜ì¸ë‰´ìŠ¤============================================
    url = 'https://www.abc.es/deportes/futbol'
    apikey = myZenrows
    params = {
        'url': url,
        'apikey': myZenrows,
    }
    spain_new = requests.get('https://api.zenrows.com/v1/', params=params)
    spain_all_data = spain_new.text
    myparser = BeautifulSoup(spain_all_data, 'html.parser')
    spain_articles_area = myparser.select("div.voc-article-content")
    for v in spain_articles_area:
        spain_news_title = v.select_one("h2.voc-title > a").text
        spain_news_full_link = v.select_one("h2.voc-title > a").get("href")
        spa_newsList.append(spain_news_title)
        spa_newsList = spa_newsList[:10]        
        spa_newsList_url.append(spain_news_full_link)
        spa_newsList_url = spa_newsList_url[:10]    

    #ì´íƒˆë¦¬ì•„ë‰´ìŠ¤============================================
    italy_new = "https://gianlucadimarzio.com/"
    italy_get_info = requests.get(italy_new)
    italy_all_data = italy_get_info.text
    myparser = BeautifulSoup(italy_all_data, 'html.parser')
    italy_articles_area = myparser.select("h3.card-title.card-title-homogenize-height.d-block")
    for v in italy_articles_area:
        italy_news_title = v.text
        italy_full_link = v.select_one("a.text-decoration-none").get("href")
        ita_newsList.append(italy_news_title)
        ita_newsList = ita_newsList[:10]   
        ita_newsList_url.append(italy_full_link)
        ita_newsList_url = ita_newsList_url[:10]

    now = datetime.now()
    global update_time
    update_time = now.strftime('%Y-%m-%d %H:%M:%S')
    
    return eng_newsList, ger_newsList, spa_newsList, ita_newsList, eng_newsList_url, ger_newsList_url, spa_newsList_url, ita_newsList_url, update_time

#Langchain ê¸°ì‚¬ìš”ì•½í•¨ìˆ˜
def summary_news(x):
    global summary_results_list
    summary_results_list = []  # ìš”ì•½ëœ ë‚´ìš©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    #ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ chunì‘ì—…
    # â”” chunk_size : í…ìŠ¤íŠ¸ ë¶„í•  í›„, ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸°
    # â”” chunk_overlap : ê° ì²­í¬ ê°„ì— ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ ì§€ì •
    # â”” length_function : í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
    # â”” is_separator_regex : êµ¬ë¶„ìë¥¼ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼í•˜ëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€
    #ì›¹ì‚¬ì´íŠ¸ ë‚´ìš© í¬ë¡¤ë§ í›„ chunk ë‹¨ìœ„ë¡œ ë¶„í• 
    for i in range(10):
        WebBaseLoader(x[i])
        text_splitter = CharacterTextSplitter(
            separator="\n\n",
            chunk_size=25000,
            chunk_overlap=2500,
            length_function=len,
            is_separator_regex=False
        )

        docs = WebBaseLoader(x[i]).load_and_split(text_splitter)

        #chunk ì‘ì—… ë‹¨ìœ„ ë³„ Template ì§€ì •
        template = '''ë„ˆëŠ” ìœ ëŸ½ ì¶•êµ¬ë¦¬ê·¸ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì•„ëŠ” ì¶•êµ¬ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜:
        {text}
        '''
        Kr_arrange_template = '''{text}
        ìš”ì•½ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
        ------------------------------------------------------------------------------
        \n(1) ê¸°ì‚¬ì œëª© : ìµœëŒ€ 20ì
        \n(2) ê¸°ì‚¬ë‚´ìš© : ê¸°ì‚¬ ìµœëŒ€ ë‘ì¤„ê¹Œì§€ ìš”ì•½í•œ ë‚´ìš©
        \n(3) ì‘ì„±ì : ê¸°ì‚¬ë¥¼ ì‘ì„±í•œ ì´ë¦„
        \n(4) ì‘ì„±ì¼ì : (í•´ë‹¹ ê¸°ì‚¬ê°€ ë“±ë¡ëœ ë…„ë„, ì›”, ì¼) ex)yyyy-mm-dd
        \n
        ------------------------------------------------------------------------------
        '''

        # Prompt í…œí”Œë¦¿
        sub_prompt = PromptTemplate(template=template, input_variables=["text"])
        last_prompt = PromptTemplate(template=Kr_arrange_template, input_variables=["text"])

        # llm ëª¨ë¸ ê°ì²´ ìƒì„±
        llm = ChatOpenAI(temperature = 0,
                        model_name = "gpt-3.5-turbo-0125",
                        api_key = OPENAI_API_KEY)
        
        # ìš”ì•½ì •ì˜
        chain = load_summarize_chain(
            llm,
            map_prompt=sub_prompt,
            combine_prompt=last_prompt,
            chain_type="map_reduce",
            verbose=False
        )
        global final_contents
        final_contents = chain.invoke(docs)
        summary_results_list.append(final_contents)

for seconds in range(600):
    main()
    time.sleep(10)

#Streamlit í—¤ë”
st.set_page_config(layout="wide")

st.header("ìœ ëŸ½ 4ê°œêµ­ ì¶•êµ¬ê¸°ì‚¬ RSS(Rich Site Summary) ì„œë¹„ìŠ¤ ğŸŒ")
st.markdown("Open AI ê¸°ë°˜ì˜ ì˜êµ­, ë…ì¼, ìŠ¤í˜ì¸, ì´íƒˆë¦¬ì•„ ì¶•êµ¬ë¦¬ê·¸ ì‹¤ì‹œê°„ ê¸°ì‚¬ë¥¼ ë²ˆì—­/ìš”ì•½í•  ìˆ˜ ìˆëŠ” í˜ì´ì§€ì„(**ê¸°ì‚¬ëŠ” 1ì‹œê°„ ì£¼ê¸°ë¡œ ê°±ì‹ ë¨**).")

#Streamlit ë°”ë””
st.markdown(f"ì—…ë°ì´íŠ¸ ì‹œê°„ : {update_time}")
tab_1, tab_2, tab_3, tab_4 = st.tabs(["ì˜êµ­ ì¶•êµ¬ NewsğŸ“œ", "ë…ì¼ ì¶•êµ¬ NewsğŸ“œ", "ìŠ¤í˜ì¸ ì¶•êµ¬ NewsğŸ“œ", "ì´íƒˆë¦¬ì•„ ì¶•êµ¬ NewsğŸ“œ"])

# â”” ì˜êµ­ë‰´ìŠ¤
with tab_1:
    eng_title= eng_newsList
    eng_link = eng_newsList_url
    eng_area = pd.DataFrame({
        "ê¸°ì‚¬ëª…": eng_title,
        "ë§í¬ ë°”ë¡œê°€ê¸°":eng_link,
    })
    st.dataframe(eng_area, use_container_width=True, hide_index=True)

    if st.button("ê¸°ì‚¬ ìš”ì•½", use_container_width=True, key="sm_1"):
        summary_news(eng_newsList_url)
        for v in range(len(summary_results_list)):
            st.markdown(summary_results_list[v]["output_text"])

# â”” ë…ì¼ë‰´ìŠ¤
with tab_2:
    ger_title= ger_newsList
    ger_link = ger_newsList_url
    ger_area = pd.DataFrame({
        "ê¸°ì‚¬ëª…": ger_title,
        "ë§í¬ ë°”ë¡œê°€ê¸°": ger_link,
    })
    st.dataframe(ger_area, use_container_width=True, hide_index=True)

    if st.button("ê¸°ì‚¬ ìš”ì•½", use_container_width=True, key="sm_2"):
        summary_news(ger_newsList_url)
        for v in range(len(summary_results_list)):
            st.markdown(summary_results_list[v]["output_text"])

# â”” ìŠ¤í˜ì¸ë‰´ìŠ¤
with tab_3:
    spa_title= spa_newsList
    spa_link = spa_newsList_url
    spa_area = pd.DataFrame({
        "ê¸°ì‚¬ëª…": spa_title,
        "ë§í¬ ë°”ë¡œê°€ê¸°":spa_link,
    })
    st.dataframe(spa_area, use_container_width=True, hide_index=True)

    if st.button("ê¸°ì‚¬ ìš”ì•½", use_container_width=True, key="sm_3"):
        summary_news(spa_newsList_url)
        for v in range(len(summary_results_list)):
            st.markdown(summary_results_list[v]["output_text"])

# â”” ì´íƒˆë¦¬ì•„ ë‰´ìŠ¤
with tab_4:
    ita_title= ita_newsList
    ita_link = ita_newsList_url
    ita_area = pd.DataFrame({
        "ê¸°ì‚¬ëª…": ita_title,
        "ë§í¬ ë°”ë¡œê°€ê¸°": ita_link,
    })
    st.dataframe(ita_area, use_container_width=True, hide_index=True)

    if st.button("ê¸°ì‚¬ ìš”ì•½", use_container_width=True, key="sm_4"):
        summary_news(ita_newsList_url)
        for v in range(len(summary_results_list)):
            st.markdown(summary_results_list[v]["output_text"])
